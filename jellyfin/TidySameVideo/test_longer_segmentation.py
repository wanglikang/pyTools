import os
import sys
from context import VideoOrganizerContext

# åˆ›å»ºæµ‹è¯•ç›®å½•ä½œä¸ºè¾“å‡ºç›®å½•
test_dir = os.path.join(os.path.dirname(__file__), 'test_output')
os.makedirs(test_dir, exist_ok=True)

def test_longer_keywords_segmentation():
    """
    æµ‹è¯•ä¼˜åŒ–åçš„åˆ†è¯åŠŸèƒ½ï¼ŒéªŒè¯æ›´é•¿å…³é”®å­—çš„æå–
    """
    print("å¼€å§‹æµ‹è¯•æ›´é•¿å…³é”®å­—çš„åˆ†è¯åŠŸèƒ½...")
    
    # åˆ›å»ºä¸Šä¸‹æ–‡å¯¹è±¡
    context = VideoOrganizerContext(output_dir=test_dir)
    
    # æµ‹è¯•ç”¨ä¾‹ï¼šå„ç§æ··åˆæ–‡æœ¬
    test_cases = [
        "å¤ä»‡è€…è”ç›Ÿ4ç»ˆå±€ä¹‹æˆ˜AvengersEndgame2019",
        "æµæµªåœ°çƒTheWanderingEarth2019ç§‘å¹»å¤§ç‰‡",
        "é¬¼ç­ä¹‹åˆƒå‰§åœºç‰ˆæ— é™åˆ—è½¦ç¯‡DemonSlayer2020",
        "è‚–ç”³å…‹çš„æ•‘èµTheShawshankRedemption1994ç»å…¸",
        "é»‘å®¢å¸å›½TheMatrixNeoTrinity1999åŠ¨ä½œç§‘å¹»",
        "åƒä¸åƒå¯»SpiritedAwayå®«å´éª2001åŠ¨ç”»",
        "æ˜Ÿé™…ç©¿è¶ŠInterstellarCooperMurph2014è¯ºå…°",
        "è¿™ä¸ªæ€æ‰‹ä¸å¤ªå†·LÃ©onMathilda1994åŠ¨ä½œå‰§æƒ…",
        "ç–¯ç‹‚åŠ¨ç‰©åŸZootopiaNickJudy2016è¿ªå£«å°¼åŠ¨ç”»",
        "ç›—æ¢¦ç©ºé—´InceptionCobbMal2010ç§‘å¹»æ‚¬ç–‘"
    ]
    
    # è®°å½•ç»“æœ
    results = []
    
    for i, test_text in enumerate(test_cases):
        print(f"\næµ‹è¯•ç”¨ä¾‹ {i+1}: {test_text}")
        keywords = context._segment_text(test_text)
        
        # ç»Ÿè®¡å…³é”®å­—é•¿åº¦ä¿¡æ¯
        keyword_lengths = [len(keyword) for keyword in keywords]
        avg_length = sum(keyword_lengths) / len(keyword_lengths) if keyword_lengths else 0
        max_length = max(keyword_lengths) if keyword_lengths else 0
        
        print(f"åˆ†è¯ç»“æœ: {keywords}")
        print(f"å…³é”®å­—æ•°é‡: {len(keywords)}")
        print(f"å¹³å‡é•¿åº¦: {avg_length:.2f}")
        print(f"æœ€å¤§é•¿åº¦: {max_length}")
        
        # æ£€æŸ¥æ˜¯å¦æå–äº†è¾ƒé•¿çš„å…³é”®å­—
        has_long_keywords = max_length >= 4
        print(f"æ˜¯å¦æå–äº†è¾ƒé•¿å…³é”®å­—(>=4å­—ç¬¦): {has_long_keywords}")
        
        results.append({
            'text': test_text,
            'keywords': keywords,
            'keyword_count': len(keywords),
            'avg_length': avg_length,
            'max_length': max_length,
            'has_long_keywords': has_long_keywords
        })
    
    # æ€»ç»“
    print("\n" + "="*50)
    print("æµ‹è¯•æ€»ç»“:")
    all_has_long = all(r['has_long_keywords'] for r in results)
    avg_keyword_count = sum(r['keyword_count'] for r in results) / len(results)
    avg_max_length = sum(r['max_length'] for r in results) / len(results)
    
    print(f"æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹æ˜¯å¦éƒ½æå–äº†è¾ƒé•¿å…³é”®å­—: {all_has_long}")
    print(f"å¹³å‡å…³é”®å­—æ•°é‡: {avg_keyword_count:.2f}")
    print(f"å¹³å‡æœ€å¤§å…³é”®å­—é•¿åº¦: {avg_max_length:.2f}")
    
    if all_has_long:
        print("\nâœ… æµ‹è¯•é€šè¿‡: åˆ†è¯ç®—æ³•æˆåŠŸæå–äº†è¾ƒé•¿çš„å…³é”®å­—")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥: éƒ¨åˆ†æµ‹è¯•ç”¨ä¾‹æœªèƒ½æå–è¶³å¤Ÿé•¿çš„å…³é”®å­—")
        for r in results:
            if not r['has_long_keywords']:
                print(f"  - å¤±è´¥ç”¨ä¾‹: {r['text']}, æœ€å¤§é•¿åº¦: {r['max_length']}")
    
    return all_has_long

def test_practical_file_matching():
    """
    æµ‹è¯•å®é™…çš„æ–‡ä»¶åŒ¹é…åœºæ™¯ï¼ŒéªŒè¯åˆ†è¯ä¼˜åŒ–å¯¹ç›¸ä¼¼æ–‡ä»¶æŸ¥æ‰¾çš„å½±å“
    """
    print("\n" + "="*50)
    print("å¼€å§‹æµ‹è¯•å®é™…æ–‡ä»¶åŒ¹é…åœºæ™¯...")
    
    # åˆ›å»ºä¸Šä¸‹æ–‡å¯¹è±¡
    context = VideoOrganizerContext(output_dir=test_dir)
    
    # å¯¼å…¥are_files_similarå‡½æ•°è¿›è¡Œç›´æ¥æµ‹è¯•
    from data_processor import are_files_similar
    
    # æµ‹è¯•ç›´æ¥çš„ç›¸ä¼¼åº¦è®¡ç®—
    print("æµ‹è¯•ç›´æ¥ç›¸ä¼¼åº¦è®¡ç®—:")
    
    # æµ‹è¯•ç”¨ä¾‹1ï¼šå¤ä»‡è€…è”ç›Ÿç›¸å…³
    file1 = {'name': 'å¤ä»‡è€…è”ç›Ÿ4ç»ˆå±€ä¹‹æˆ˜AvengersEndgame2019.mp4', 'size': 1000000000, 'directory': 'movies1'}
    file2 = {'name': 'å¤ä»‡è€…è”ç›Ÿ4.Endgame.2019.BD1080p.mp4', 'size': 800000000, 'directory': 'movies2'}
    result1 = are_files_similar(file1, file2, context=context)
    print(f"æµ‹è¯•ç”¨ä¾‹1 (å¤ä»‡è€…è”ç›Ÿ): {'ç›¸ä¼¼' if result1 else 'ä¸ç›¸ä¼¼'}")
    
    # æµ‹è¯•ç”¨ä¾‹2ï¼šæµæµªåœ°çƒç›¸å…³
    file3 = {'name': 'æµæµªåœ°çƒTheWanderingEarth2019.mp4', 'size': 900000000, 'directory': 'movies1'}
    file4 = {'name': 'æµæµªåœ°çƒ.2019.ç§‘å¹»å¤§ç‰‡.HD.mp4', 'size': 700000000, 'directory': 'movies2'}
    result2 = are_files_similar(file3, file4, context=context)
    print(f"æµ‹è¯•ç”¨ä¾‹2 (æµæµªåœ°çƒ): {'ç›¸ä¼¼' if result2 else 'ä¸ç›¸ä¼¼'}")
    
    # æµ‹è¯•ç”¨ä¾‹3ï¼šé¬¼ç­ä¹‹åˆƒç›¸å…³ï¼ˆä¸­æ–‡å’Œè‹±æ–‡åç§°ï¼‰
    file5 = {'name': 'é¬¼ç­ä¹‹åˆƒå‰§åœºç‰ˆæ— é™åˆ—è½¦ç¯‡.mp4', 'size': 600000000, 'directory': 'anime1'}
    file6 = {'name': 'Demon.Slayer.Mugen.Train.2020.mp4', 'size': 500000000, 'directory': 'anime2'}
    result3 = are_files_similar(file5, file6, context=context)
    print(f"æµ‹è¯•ç”¨ä¾‹3 (é¬¼ç­ä¹‹åˆƒ): {'ç›¸ä¼¼' if result3 else 'ä¸ç›¸ä¼¼'}")
    
    # åˆ†è¯éªŒè¯ - æ˜¾ç¤ºæ¯ä¸ªæµ‹è¯•ç”¨ä¾‹çš„åˆ†è¯ç»“æœ
    print("\nåˆ†è¯ç»“æœéªŒè¯:")
    
    # æµ‹è¯•ç”¨ä¾‹1çš„åˆ†è¯
    keywords1_1 = context._segment_text(os.path.splitext(file1['name'])[0])
    keywords1_2 = context._segment_text(os.path.splitext(file2['name'])[0])
    print(f"\nå¤ä»‡è€…è”ç›Ÿæ–‡ä»¶1åˆ†è¯: {keywords1_1}")
    print(f"å¤ä»‡è€…è”ç›Ÿæ–‡ä»¶2åˆ†è¯: {keywords1_2}")
    common_keywords1 = set(keywords1_1) & set(keywords1_2)
    print(f"å…±åŒå…³é”®å­—: {common_keywords1}")
    
    # æµ‹è¯•ç”¨ä¾‹2çš„åˆ†è¯
    keywords2_1 = context._segment_text(os.path.splitext(file3['name'])[0])
    keywords2_2 = context._segment_text(os.path.splitext(file4['name'])[0])
    print(f"\næµæµªåœ°çƒæ–‡ä»¶1åˆ†è¯: {keywords2_1}")
    print(f"æµæµªåœ°çƒæ–‡ä»¶2åˆ†è¯: {keywords2_2}")
    common_keywords2 = set(keywords2_1) & set(keywords2_2)
    print(f"å…±åŒå…³é”®å­—: {common_keywords2}")
    
    # æ¨¡æ‹Ÿæ–‡ä»¶åˆ—è¡¨ç”¨äºå€’æ’ç´¢å¼•æµ‹è¯•
    mock_files = [file1, file2, file3, file4, file5, file6]
    for i, f in enumerate(mock_files):
        f['path'] = f'movie{i+1}.mp4'
    
    # è®¾ç½®æ–‡ä»¶åˆ—è¡¨
    context.update_file_list(mock_files)
    
    # åˆ›å»ºå€’æ’ç´¢å¼•
    index = context.create_inverted_index()
    print(f"\nåˆ›å»ºçš„å€’æ’ç´¢å¼•åŒ…å« {len(index)} ä¸ªå…³é”®è¯")
    
    # æ˜¾ç¤ºéƒ¨åˆ†ç´¢å¼•å†…å®¹
    print("å€’æ’ç´¢å¼•éƒ¨åˆ†å†…å®¹:")
    sorted_keywords = sorted(index.keys(), key=len, reverse=True)
    for i, keyword in enumerate(sorted_keywords[:5]):  # æ˜¾ç¤ºå‰5ä¸ªæœ€é•¿çš„å…³é”®å­—
        file_count = len(index[keyword])
        print(f"  '{keyword}' ({len(keyword)}å­—ç¬¦): {file_count}ä¸ªæ–‡ä»¶")
    
    # å¯¼å…¥find_similar_file_groupså‡½æ•°
    from data_processor import find_similar_file_groups
    
    # æŸ¥æ‰¾ç›¸ä¼¼æ–‡ä»¶ç»„
    similar_groups = find_similar_file_groups(context)
    print(f"\næ‰¾åˆ° {len(similar_groups)} ç»„ç›¸ä¼¼æ–‡ä»¶")
    
    # æ˜¾ç¤ºç›¸ä¼¼æ–‡ä»¶ç»„
    for i, group in enumerate(similar_groups):
        print(f"\nç›¸ä¼¼ç»„ {i+1}:")
        for file_info in group:
            print(f"  - {file_info['name']}")
    
    # éªŒè¯åŒ¹é…ç»“æœ
    expected_matches = 2  # æœŸæœ›å¤ä»‡è€…è”ç›Ÿå’Œæµæµªåœ°çƒåŒ¹é…ï¼Œé¬¼ç­ä¹‹åˆƒä¸­è‹±æ–‡å¯èƒ½éš¾ä»¥åŒ¹é…
    success = result1 and result2 and len(similar_groups) >= expected_matches
    
    if success:
        print("\nâœ… æµ‹è¯•é€šè¿‡: ç›¸ä¼¼åº¦è®¡ç®—å’Œæ–‡ä»¶åŒ¹é…æ­£å¸¸å·¥ä½œ")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥: ç›¸ä¼¼åº¦è®¡ç®—æˆ–æ–‡ä»¶åŒ¹é…å­˜åœ¨é—®é¢˜")
    
    return success

def main():
    """
    è¿è¡Œæ‰€æœ‰æµ‹è¯•
    """
    print("åˆ†è¯ä¼˜åŒ–æµ‹è¯•è„šæœ¬")
    print("="*50)
    
    # è¿è¡Œåˆ†è¯æµ‹è¯•
    segmentation_success = test_longer_keywords_segmentation()
    
    # è¿è¡Œæ–‡ä»¶åŒ¹é…æµ‹è¯•
    matching_success = test_practical_file_matching()
    
    print("\n" + "="*50)
    print("æ•´ä½“æµ‹è¯•ç»“æœ:")
    
    if segmentation_success and matching_success:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥")
        return 1

if __name__ == "__main__":
    sys.exit(main())